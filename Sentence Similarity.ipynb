{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence similarity\n",
    "Compare a incoming sentence to a list of candidate sentences and return a most similar one.  \n",
    "Here we use the pre-trained word2vec from Google. \n",
    "\n",
    "Reference:  \n",
    "1.[Gensim package tutorial](https://radimrehurek.com/gensim/index.html)  \n",
    "2.Google pretrained model can be download from [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit)  \n",
    "3.[Official website for gensim.](https://code.google.com/archive/p/word2vec/)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation: load google pre-trained word2vec model\n",
    "This pre-trained model consist of 3 million words. This size of 'GoogleNews-vectors-negative300.bin' is more than 3 gigabytes. For usage convenient, here we extract the word2vec, normalize them, and save it to a pickle file. This step only need to run once. As long as we generate the vocab_dict.pickle file, the following code can be ignored.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import gensim\n",
    "import pickle\n",
    "from scipy.linalg import norm\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "vocab = set(model.wv.vocab)\n",
    "\n",
    "vocab_dict = dict()\n",
    "for word in vocab:\n",
    "    try:\n",
    "        tmp = model.wv[word] \n",
    "        vocab_dict[word] = tmp / norm(tmp)\n",
    "    except:\n",
    "        print(word)\n",
    "\n",
    "\n",
    "with open('vocab_dict.pickle','wb') as f:\n",
    "    pickle.dump(vocab_dict, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Start from Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "wordnet = WordNetLemmatizer()\n",
    "import operator\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pickle\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuerySimilarSentence:\n",
    "    '''\n",
    "    Given a list of sentences and then input a new one, return a most similar sentence from the list\n",
    "    '''\n",
    "    def __init__(self, candidates, vocab_dict_path='vocab_dict.pickle'):\n",
    "        '''\n",
    "        Arguments:\n",
    "            cadidates: a list of strings\n",
    "            vocab_dict_path: the path of vocab_dict.pickle file\n",
    "        '''\n",
    "        with open(vocab_dict_path,'rb') as f:\n",
    "            self.vocab_dict = pickle.load(f)\n",
    "        self.vocab = set(self.vocab_dict.keys())\n",
    "        # stop words\n",
    "        self.stop_words = set(stopwords.words('english')) \n",
    "        # tokenize the sentences\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        # candidate sentences' vector\n",
    "        self.candidates = candidates\n",
    "        self.candidates_vecs = [self.str_to_vec(s) for s in candidates]  \n",
    "    \n",
    "    def add_candidate(candidates):\n",
    "        self.candidates += list(candidates)\n",
    "        self.candidates_vecs += [self.str_to_vec(s) for s in list(candidates)]\n",
    "    \n",
    "    def update_candidate(candidates):\n",
    "        self.candidates = list(candidates)\n",
    "        self.candidates_vecs = [self.str_to_vec(s) for s in list(candidates)]\n",
    "        \n",
    "    def str_to_vec(self, s):\n",
    "        '''\n",
    "        Convert a string to a 2d vector\n",
    "        Arguments:\n",
    "            s: sentence string\n",
    "        Return:\n",
    "            vec: a 2D numpy vector\n",
    "        '''\n",
    "        s = self.tokenizer.tokenize(s)\n",
    "        # lemmetize the word, for example: things -> thing, thinking ->think\n",
    "        s = [wordnet.lemmatize(w.lower()) for w in s]\n",
    "        # only keep the word in the vocabulary and non-stop word\n",
    "        s = [w for w in s if w in self.vocab and w not in self.stop_words]  \n",
    "        vec = np.array([self.vocab_dict[w] for w in s])\n",
    "        return vec\n",
    "\n",
    "    def get_sim(self, vec_1, vec_2):\n",
    "        '''\n",
    "        Compute the consine similarity between two 2d numpy vectors\n",
    "        Arguments:\n",
    "            vec_1,vec_2: 2D numpy vector\n",
    "        Return:\n",
    "            res: numerical number, similarity between vec_1 and vec_2\n",
    "        '''\n",
    "        return np.mean(np.max(np.dot(vec_1, vec_2.T),axis=1))\n",
    "    \n",
    "    def query(self, s):\n",
    "        '''\n",
    "        Input a sentence and query the most similar sentence in the candidates\n",
    "        Arguments:\n",
    "            s: string, input sentence\n",
    "        Return:\n",
    "            res: the most similar sentence in the candidate list\n",
    "        '''\n",
    "        s_vec = self.str_to_vec(s)\n",
    "        # compute the similarity\n",
    "        similarities = [self.get_sim(s_vec,c) for c in self.candidates_vecs]\n",
    "        return self.candidates[np.argmax(similarities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [\"\"\"What's the temperature today?\"\"\",\"\"\"How is the traffic?\"\"\",\"\"\"What's the date today?\"\"\",\"\"\"Play some music.\"\"\"]\n",
    "question = \"\"\"How about the traffic?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_s = QuerySimilarSentence(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How is the traffic?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_s.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What's the temperature today?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_s.query(\"What's the degree?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Play some music.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_s.query(\"Find a song\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model improvement\n",
    "1. The advantage of utilizing Google pre-trained model is the flexibility to different expressions and the drawback is the large memory usage (larger than 3 gigabytes).  \n",
    "2. There are three practical approaches to solve this problem:  \n",
    "    (1). The vocabulary contains 3 million words. Most of them are useless. Take them out and create a small subset vocabulary.  \n",
    "    (2). Train your own word2vec model  \n",
    "    (3). Use one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_s.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
